<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Diffusion on Language Model Encodings for Protein Sequence Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="Viacheslav Meshchaninov" target="_blank">Viacheslav Meshchaninov</a><sup>* 1</sup>,</span>
              <span class="author-block">
                <a href="Pavel Strashnov" target="_blank">Pavel Strashnov</a><sup>* 2</sup>,</span>
              <span class="author-block">
                <a href="Andrey Shevtsov" target="_blank">Andrey Shevtsov</a><sup>* 2</sup>,</span>
                <br>
              <span class="author-block">
                <a href="Fedor Nikolaev" target="_blank">Fedor Nikolaev</a><sup>* 2</sup>,</span>
              <span class="author-block">
                <a href="Nikita Ivanisenko" target="_blank">Nikita Ivanisenko</a><sup>* 2</sup>,</span>
              <span class="author-block">
                <a href="Olga Kardymon" target="_blank">Olga Kardymon</a><sup>* 2</sup>,</span>
              <span class="author-block">
                <a href="Dmitry Vetrov" target="_blank">Dmitry Vetrov</a><sup>* 1</sup></span>

                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1 </sup>Constructor University, <sup>2 </sup>AIRI<br>
                      International Conference on Machine Learning (ICML) 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv link --> 
                      <span class="link-block">
                        <a href="https://openreview.net/pdf?id=xB9eROwBCB" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MeshchaninovViacheslav/DiMA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>-->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Protein sequence design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present DiMA, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only (ESM-2, ESMc), dual-decodable (CHEAP), and multimodal (SaProt) representations using the same architecture and training approach. We conduct extensive evaluation of existing methods alongside DiMA using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. DiMA consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design, despite being trained solely on sequence data. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<h2 class="title is-3" id="Overview"> Model Overview </h2>
        <div class="content has-text-justified">
          <p>
            In this study, we develop DiMA, a new latent diffusion model that operates on protein language model representations. We demonstrate
that continuous diffusion on protein embeddings enables effective sequence and structure generation across multiple tasks and encoder architectures. 
DiMA addresses the limitations of previous continuous diffusion approaches that
have been limited to specific representations by establishing
a unified framework that generalizes across diverse protein
encoders. By operating in the continuous latent space
of these pre-trained encoders, our approach circumvents
the challenges associated with discrete sequence modeling
while maintaining the expressiveness needed for complex
protein design tasks. The framework is designed to be
encoder-agnostic, allowing it to benefit from advances in
protein representation learning without requiring architectural
modifications.
          </p>

<figure style="margin-top: 1rem; margin-bottom: 1rem;"> 
            <img src="./static/images/DIMA_ICML2025_main_figure-min.png" alt="Model Overview" style="width:120%;">
            
            <!-- Add the figcaption below the image -->
            <figcaption class="has-text-centered is-size-7 mt-2"> 
              <strong>DiMA.</strong> The framework consists of three main components: (1) a pre-trained protein language model encoder that maps amino
acid sequences to continuous latent representations, (2) a diffusion denoiser that generates latent vectors from Gaussian noise, and (3)
sequence and structure decoders that reconstruct amino acid sequences and protein structures from the generated latent representations.
During training, the model learns to denoise corrupted protein representations. During inference, the framework supports both unconditional
generation and conditional generation tasks including motif scaffolding, fold conditioning, and family-specific generation. The
approach enables joint sequence-structure generation while operating entirely in continuous latent space.</strong>
            </figcaption>
          </figure>

<!-- Paper poster 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/main_figure.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{meshchaninov2025dima,
    title={Diffusion on Language Model Encodings for Protein Sequence Generation},
    author={Meshchaninov, Viacheslav and Strashnov, Pavel and Shevtsov, Andrey and Nikolaev, Fedor and Ivanisenko, Nikita and Kardymon, Olga and Vetrov, Dmitry},
    booktitle={International Conference on Machine Learning (ICML)},
    year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
